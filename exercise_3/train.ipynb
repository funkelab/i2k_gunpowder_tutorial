{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from funlib.learn.torch.models import UNet, ConvPass\n",
    "import gunpowder as gp\n",
    "import logging\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Input/Output Shape\n",
    "\n",
    "Similar to what we did before, but this time, the input and output shape of our network is 4D!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters:\n",
    "\n",
    "# number of iterations to train for\n",
    "num_iterations = int(1e5)\n",
    "# input size of the U-Net (7 time frames)\n",
    "input_shape = (7, 64, 124, 124)\n",
    "# output size of the U-Net (1 time frame)\n",
    "output_shape = (1, 32, 32, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Loss, and Optimizer Setup\n",
    "\n",
    "Here we create a U-Net as before, but this time we use several input channels to feed different time frames into the network. We give the network seven frames and let it output a single feature map for the center frame at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model, loss, and optimizer\n",
    "\n",
    "unet = UNet(\n",
    "    in_channels=7,\n",
    "    num_fmaps=12,\n",
    "    fmap_inc_factor=5,\n",
    "    downsample_factors=[\n",
    "        (1, 2, 2),\n",
    "        (1, 2, 2),\n",
    "        (2, 2, 2)],\n",
    "    constant_upsample=True,\n",
    "    padding='valid')\n",
    "model = torch.nn.Sequential(\n",
    "    unet,\n",
    "    ConvPass(12, 1, [(1, 1, 1)], activation=None),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays and Graphs\n",
    "\n",
    "This time, we will have a new kind of data in a batch: a graph, called 'centers'. Those are the locations of cells (each cell is a node with an x, y, and z coordinate). There are no edges in this example.\n",
    "\n",
    "Since we can't train on graphs directly, we will have to convert the graph into a volumetric array. We will do that by drawing a Gaussian blob around each node in 'centers' and store the result in an array called 'blobs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare arrays and graphs to use\n",
    "\n",
    "# the raw data\n",
    "raw = gp.ArrayKey(\"RAW\")\n",
    "# point annotations for cell centers\n",
    "centers = gp.GraphKey(\"CENTERS\")\n",
    "# Gaussian blobs drawn around each cell center\n",
    "blobs = gp.ArrayKey(\"BLOBS\")\n",
    "# the prediction of the network\n",
    "prediction = gp.ArrayKey(\"PREDICTION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Data Sources\n",
    "\n",
    "This time, we have data coming from two sources: raw data from a zarr container, and a graph from a text file. Here we create one source for each, and merge both sources into one using the `MergeProvider`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data sources\n",
    "\n",
    "raw_source = gp.ZarrSource(\n",
    "    '../data/mouse.n5',\n",
    "    {raw: 'raw'},\n",
    "    {raw: gp.ArraySpec(interpolatable=True)}\n",
    ")\n",
    "centers_source = gp.CsvPointsSource(\n",
    "    '../data/mouse_cells.csv',\n",
    "    centers\n",
    ")\n",
    "\n",
    "# combine both raw and cell center source into a single provider\n",
    "sources = (raw_source, centers_source) + gp.MergeProvider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Pipeline\n",
    "\n",
    "The train pipeline is very similar to the previous exercises. The biggest differences are:\n",
    "1. We deal with 4D data now.\n",
    "2. We need to convert the graph in 'centers' into an array of Gaussian blobs in 'blobs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the voxel size of the raw data\n",
    "with gp.build(raw_source):\n",
    "    voxel_size = raw_source.spec[raw].voxel_size\n",
    "\n",
    "# from here on, all sizes are in world units\n",
    "input_size = voxel_size*input_shape\n",
    "output_size = voxel_size*output_shape\n",
    "\n",
    "# create a request for a training batch\n",
    "request = gp.BatchRequest()\n",
    "request.add(raw, input_size)\n",
    "request.add(blobs, output_size)\n",
    "\n",
    "# snapshots should contain the prediction as well\n",
    "snapshot_request = gp.BatchRequest()\n",
    "snapshot_request[prediction] = request[blobs]\n",
    "\n",
    "pipeline = (\n",
    "    \n",
    "    sources +\n",
    "    \n",
    "    # pick a random location to train on, but ensure that there is a cell in the center\n",
    "    gp.RandomLocation(ensure_nonempty=centers, ensure_centered=True) +\n",
    "    \n",
    "    # augment data using rotations and elastic deformation\n",
    "    gp.ElasticAugment(\n",
    "        control_point_spacing=(5, 10, 10),\n",
    "        jitter_sigma=(1.0, 1.0, 1.0),\n",
    "        rotation_interval=[0, math.pi/2.0],\n",
    "        subsample=8\n",
    "    ) +\n",
    "    \n",
    "    # turn cell center annotations into Gaussian blobs\n",
    "    gp.RasterizeGraph(\n",
    "        centers,\n",
    "        blobs,\n",
    "        array_spec=gp.ArraySpec(voxel_size=voxel_size, dtype=np.float32),\n",
    "        settings=gp.RasterizationSettings(\n",
    "            radius=(1, 10, 10, 10),\n",
    "            mode='peak'\n",
    "        )\n",
    "    ) +\n",
    "    \n",
    "    # introduce a \"batch\" dimension\n",
    "    gp.Stack(1) +\n",
    "    # (arrays have shape (1, t, d, h, w) now)\n",
    "    \n",
    "    # use parallel processes to pre-fetch batches from upstream\n",
    "    gp.PreCache() +\n",
    "    \n",
    "    # train the model to match the prediction with the blobs\n",
    "    gp.torch.Train(\n",
    "        model,\n",
    "        loss,\n",
    "        optimizer,\n",
    "        inputs={\n",
    "            'input': raw\n",
    "        },\n",
    "        outputs={\n",
    "            0: prediction\n",
    "        },\n",
    "        loss_inputs={\n",
    "            0: prediction,\n",
    "            1: blobs\n",
    "        },\n",
    "        save_every=10000\n",
    "    ) +\n",
    "    \n",
    "    # remove the batch dimension again\n",
    "    gp.Squeeze([raw, blobs, prediction]) +\n",
    "    # (arrays have shape (t, d, h, w) now)\n",
    "    \n",
    "    # store a training snapshot every 1000 iterations\n",
    "    gp.Snapshot({\n",
    "        raw: 'raw',\n",
    "        blobs: 'blobs',\n",
    "        prediction: 'prediction'\n",
    "    },\n",
    "    output_filename='it_{iteration}.hdf',\n",
    "    additional_request=snapshot_request,\n",
    "    every=1000)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the pipeline and train\n",
    "with gp.build(pipeline):\n",
    "    for i in range(num_iterations):\n",
    "        batch = pipeline.request_batch(request)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
