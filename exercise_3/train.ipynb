{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gunpowder' has no attribute 'RasterizeGraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ef593b627b45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# turn cell center annotations into Gaussian blobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     gp.RasterizeGraph(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mblobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gunpowder' has no attribute 'RasterizeGraph'"
     ]
    }
   ],
   "source": [
    "from funlib.learn.torch.models import UNet, ConvPass\n",
    "import gunpowder as gp\n",
    "import logging\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# training parameters:\n",
    "\n",
    "# number of iterations to train for\n",
    "num_iterations = int(1e5)\n",
    "# input size of the U-Net (7 time frames)\n",
    "input_shape = (7, 64, 124, 124)\n",
    "# output size of the U-Net (1 time frame)\n",
    "output_shape = (1, 32, 32, 32)\n",
    "\n",
    "# declare arrays and graphs to use\n",
    "\n",
    "# the raw data\n",
    "raw = gp.ArrayKey(\"RAW\")\n",
    "# point annotations for cell centers\n",
    "centers = gp.GraphKey(\"CENTERS\")\n",
    "# Gaussian blobs drawn around each cell center\n",
    "blobs = gp.ArrayKey(\"BLOBS\")\n",
    "# the prediction of the network\n",
    "prediction = gp.ArrayKey(\"PREDICTION\")\n",
    "\n",
    "# create model, loss, and optimizer\n",
    "\n",
    "unet = UNet(\n",
    "    in_channels=7,\n",
    "    num_fmaps=12,\n",
    "    fmap_inc_factor=5,\n",
    "    downsample_factors=[\n",
    "        (1, 2, 2),\n",
    "        (1, 2, 2),\n",
    "        (2, 2, 2)],\n",
    "    constant_upsample=True,\n",
    "    padding='valid')\n",
    "model = torch.nn.Sequential(\n",
    "    unet,\n",
    "    ConvPass(12, 1, [(1, 1, 1)], activation=None),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# assemble the training pipeline\n",
    "\n",
    "raw_source = gp.ZarrSource(\n",
    "    '../data/mouse.n5',\n",
    "    {raw: 'raw'},\n",
    "    {raw: gp.ArraySpec(interpolatable=True)}\n",
    ")\n",
    "centers_source = gp.CsvPointsSource(\n",
    "    '../data/mouse_cells.csv',\n",
    "    centers\n",
    ")\n",
    "\n",
    "# get the voxel size of the raw data\n",
    "with gp.build(raw_source):\n",
    "    voxel_size = raw_source.spec[raw].voxel_size\n",
    "\n",
    "# from here on, all sizes are in world units\n",
    "input_size = voxel_size*input_shape\n",
    "output_size = voxel_size*output_shape\n",
    "\n",
    "# create a request for a training batch\n",
    "request = gp.BatchRequest()\n",
    "request.add(raw, input_size)\n",
    "request.add(blobs, output_size)\n",
    "\n",
    "# snapshots should contain the prediction as well\n",
    "snapshot_request = gp.BatchRequest()\n",
    "snapshot_request[prediction] = request[blobs]\n",
    "\n",
    "pipeline = (\n",
    "    \n",
    "    # combine both raw and cell center source into a single provider\n",
    "    (raw_source, centers_source) +\n",
    "    gp.MergeProvider() +\n",
    "    \n",
    "    # pick a random location to train on, but ensure that there is a cell in the center\n",
    "    gp.RandomLocation(ensure_nonempty=centers, ensure_centered=True) +\n",
    "    \n",
    "    # augment data using rotations and elastic deformation\n",
    "    gp.ElasticAugment(\n",
    "        control_point_spacing=(5, 10, 10),\n",
    "        jitter_sigma=(1.0, 1.0, 1.0),\n",
    "        rotation_interval=[0, math.pi/2.0],\n",
    "        subsample=8\n",
    "    ) +\n",
    "    \n",
    "    # turn cell center annotations into Gaussian blobs\n",
    "    gp.RasterizeGraph(\n",
    "        centers,\n",
    "        blobs,\n",
    "        array_spec=gp.ArraySpec(voxel_size=voxel_size, dtype=np.float32),\n",
    "        settings=gp.RasterizationSettings(\n",
    "            radius=(1, 10, 10, 10),\n",
    "            mode='peak'\n",
    "        )\n",
    "    ) +\n",
    "    \n",
    "    # introduce a \"batch\" dimension\n",
    "    gp.Stack(1) +\n",
    "    # (arrays have shape (1, t, d, h, w) now)\n",
    "    \n",
    "    # use parallel processes to pre-fetch batches from upstream\n",
    "    gp.PreCache() +\n",
    "    \n",
    "    # train the model to match the prediction with the blobs\n",
    "    gp.torch.Train(\n",
    "        model,\n",
    "        loss,\n",
    "        optimizer,\n",
    "        inputs={\n",
    "            'input': raw\n",
    "        },\n",
    "        outputs={\n",
    "            0: prediction\n",
    "        },\n",
    "        loss_inputs={\n",
    "            0: prediction,\n",
    "            1: blobs\n",
    "        },\n",
    "        save_every=10000\n",
    "    ) +\n",
    "    \n",
    "    # remove the batch dimension again\n",
    "    gp.Squeeze([raw, blobs, prediction]) +\n",
    "    # (arrays have shape (t, d, h, w) now)\n",
    "    \n",
    "    # store a training snapshot every 1000 iterations\n",
    "    gp.Snapshot({\n",
    "        raw: 'raw',\n",
    "        blobs: 'blobs',\n",
    "        prediction: 'prediction'\n",
    "    },\n",
    "    output_filename='it_{iteration}.hdf',\n",
    "    additional_request=snapshot_request,\n",
    "    every=1000)\n",
    ")\n",
    "\n",
    "# build the pipeline and train\n",
    "with gp.build(pipeline):\n",
    "    for i in range(num_iterations):\n",
    "        batch = pipeline.request_batch(request)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
